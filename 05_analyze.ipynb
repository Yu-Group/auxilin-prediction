{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "plt.style.use('dark_background')\n",
    "import mat4py\n",
    "import pandas as pd\n",
    "import data_tracks\n",
    "import models\n",
    "from matplotlib_venn import venn3, venn2\n",
    "from sklearn.model_selection import KFold\n",
    "from colorama import Fore\n",
    "import pickle as pkl\n",
    "import viz\n",
    "from style import *\n",
    "import analyze_helper\n",
    "import data_tracks\n",
    "import train\n",
    "\n",
    "outcome_def = 'y_consec_sig'\n",
    "out_dir = 'results/outcome=y_consec_sig'\n",
    "results = analyze_helper.load_results(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at prediction metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "r = r[[k for k in r if not '_f' in k]]\n",
    "# r = r[r.index.str.contains('ros')] # only use random sampling\n",
    "r = r.sort_values(by=['accuracy', 'balanced_accuracy'], ascending=False)\n",
    "# r.style.background_gradient(cmap='viridis', axis=None) # all values on same cmap\n",
    "r.style.background_gradient(cmap='viridis', axis=0) # columns differently colored\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at feat importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])\n",
    "r = r[r.accuracy > 0.65]\n",
    "keys = [k for k in r if '_f' in k]\n",
    "keys_remapped = {k: k.replace('_f', '') for k in keys}\n",
    "r = r[keys].rename(columns=keys_remapped)\n",
    "# r = r.sort_values('lifetime')\n",
    "# r = r[r.index.str.contains('39')]\n",
    "r = r[~r.index.str.contains('=11')]\n",
    "# r = r[~r.index.str.contains('=7')]\n",
    "r = r[~r.index.str.contains('=15')]\n",
    "r = r.rename(columns={'mean_square_displacement': 'msd', 'total_displacement': 'td'})\n",
    "\n",
    "# r = r[r.index.str.contains('11')]\n",
    "# r = r[r2.index.str.contains('ros')]\n",
    "# r = r[r.index.str.contains('none')]\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "\n",
    "\n",
    "def rank(r):\n",
    "    '''Rank feature importances appropriately\n",
    "    '''\n",
    "    r = r.abs()\n",
    "    r = r.rank(axis=1, ascending=False, method='min')\n",
    "    return r\n",
    "# \n",
    "r = rank(r)\n",
    "r = r.reindex(r.mean().sort_values(ascending=True).index, axis=1) # sort cols by mean rank\n",
    "\n",
    "idxs = r.index\n",
    "r.insert(0, 'acc', results.loc[idxs]['accuracy'])\n",
    "r = r.sort_values('acc', ascending=False)\n",
    "\n",
    "subset = list(r.keys())\n",
    "subset.remove('acc')\n",
    "r.fillna(0).style.background_gradient(cmap='viridis_r', axis=1, subset=subset) # rows differently colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mlp2_15_ros_select_rf=3' 'svm_25_ros_select_rf=11'\n",
      " 'svm_25_ros_select_rf=5' 'mlp2_45_ros_select_rf=7'\n",
      " 'svm_15_ros_select_rf=5' 'mlp2_25_ros_select_rf=7'\n",
      " 'mlp2_25_ros_select_rf=3' 'svm_25_ros_select_rf=3'\n",
      " 'svm_45_ros_select_rf=5' 'svm_15_ros_select_rf=11']\n"
     ]
    }
   ],
   "source": [
    "df = data_tracks.get_data()\n",
    "n = df.shape[0]\n",
    "\n",
    "# normalize and store\n",
    "X = df[data_tracks.get_feature_names(df)]\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "ks = list(X.keys())\n",
    "norms = {ks[i]: {'mu': X_mean[i], 'std': X_std[i]} for i in range(len(ks))}\n",
    "X = (X - X_mean) / X_std\n",
    "y = df[outcome_def].values\n",
    "\n",
    "\n",
    "# split testing data based on cell num\n",
    "cv_idx = data_tracks.cell_nums_train[0]\n",
    "idxs_test = df.cell_num.isin([cv_idx]) # this is the cv set for model's trained in the first cv fold\n",
    "X_test, Y_test = X[idxs_test], y[idxs_test]\n",
    "\n",
    "# look at best models\n",
    "r = results\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "idx = np.array(r.index)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_full_cv, idxs_cv = analyze_helper.get_data_over_folds(model_name, out_dir, df.cell_num, X, y)\n",
    "y_full_cv = df[outcome_def].iloc[idxs_cv].values\n",
    "preds = d_full_cv[model_name].values\n",
    "preds_proba = d_full_cv[model_name + '_proba'].values\n",
    "\n",
    "results_individual = pkl.load(open(oj(out_dir, f'{model_name}.pkl'), 'rb'))\n",
    "assert np.mean(preds==y_full_cv) == np.average(results_individual['cv']['accuracy'], weights=results_individual['num_pts_by_fold_cv']), 'did not properly load model/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_lifetime(X.iloc[idxs_cv], preds, preds_proba, y_full_cv, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_spatially(df, idxs_cv, preds, y_full_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_outliers(X.iloc[idxs_cv], preds, y_full_cv, num_feats_reduced=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_fp = preds > y_full_cv\n",
    "idxs_fn = preds < y_full_cv\n",
    "print('fp')\n",
    "viz.viz_biggest_errs(df['X'].iloc[idxs_cv][idxs_fp], y_full_cv[idxs_fp], preds[idxs_fp], preds_proba[idxs_fp])    \n",
    "print('fn')\n",
    "viz.viz_biggest_errs(df['X'].iloc[idxs_cv][idxs_fn], y_full_cv[idxs_fn], preds[idxs_fn], preds_proba[idxs_fn])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp2_15_ros_select_rf=3\n",
      "svm_25_ros_select_rf=11\n",
      "svm_25_ros_select_rf=5\n"
     ]
    }
   ],
   "source": [
    "# model_names = ['mlp2_11_none', 'svm_35_none', 'logistic_4_none', 'rf_9_none']\n",
    "# model_names = ['mlp2_11_none', 'mlp2_9_none', 'mlp2_4_none']\n",
    "model_names = idx[:3]\n",
    "d_full_cv, idxs_cv = analyze_helper.get_data_over_folds(model_names, out_dir, df.cell_num, X, y)\n",
    "y_full_cv = df[outcome_def].iloc[idxs_cv].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**venn-diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = []\n",
    "for model_name in model_names:\n",
    "    args = np.argwhere(d_full_cv[model_name] != y_full_cv)\n",
    "    sets.append(set(args.flatten().tolist()))\n",
    "    \n",
    "plt.figure(dpi=300)\n",
    "plt.title('venn diagram of shared errors')\n",
    "venn3(sets, model_names[:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ensemble err**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.070164\n",
      "1       0.840049\n",
      "2       0.088628\n",
      "3       0.120895\n",
      "4       0.905893\n",
      "5       0.129751\n",
      "6       0.510093\n",
      "7       0.140856\n",
      "8       0.114931\n",
      "9       0.517429\n",
      "10      0.124169\n",
      "11      0.125611\n",
      "12      0.922058\n",
      "13      0.074506\n",
      "14      0.068827\n",
      "15      0.040415\n",
      "16      0.860703\n",
      "17      0.909912\n",
      "18      0.541398\n",
      "19      0.123836\n",
      "20      0.851159\n",
      "21      0.147195\n",
      "22      0.896946\n",
      "23      0.134209\n",
      "24      0.137617\n",
      "25      0.136420\n",
      "26      0.155908\n",
      "27      0.861863\n",
      "28      0.893473\n",
      "29      0.895077\n",
      "          ...   \n",
      "1003    0.066882\n",
      "1004    0.052670\n",
      "1005    0.565268\n",
      "1006    0.112738\n",
      "1007    0.910013\n",
      "1008    0.164609\n",
      "1009    0.198462\n",
      "1010    0.055731\n",
      "1011    0.105055\n",
      "1012    0.876831\n",
      "1013    0.168407\n",
      "1014    0.125791\n",
      "1015    0.189192\n",
      "1016    0.906719\n",
      "1017    0.875765\n",
      "1018    0.918448\n",
      "1019    0.914523\n",
      "1020    0.098564\n",
      "1021    0.894506\n",
      "1022    0.118730\n",
      "1023    0.149869\n",
      "1024    0.098325\n",
      "1025    0.079775\n",
      "1026    0.067586\n",
      "1027    0.071098\n",
      "1028    0.891601\n",
      "1029    0.913778\n",
      "1030    0.068958\n",
      "1031    0.959682\n",
      "1032    0.121664\n",
      "Length: 1033, dtype: float64\n",
      "balanced_accuracy 0.0\n",
      "accuracy 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/vision/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1855: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn('y_pred contains classes not in y_true')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-ee47a47060a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mscore_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscorers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscore_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_full_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds_soft\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# metrics.accuracy_score(y_ensemble, preds_soft>0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1666\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'precision'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m                                                  \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1668\u001b[0;31m                                                  zero_division=zero_division)\n\u001b[0m\u001b[1;32m   1669\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1478\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0;32m-> 1480\u001b[0;31m                                     pos_label)\n\u001b[0m\u001b[1;32m   1481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1310\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m   1311\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1312\u001b[0;31m                              % (y_type, average_options))\n\u001b[0m\u001b[1;32m   1313\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "d_full_cv, y_full_cv = analyze_helper.get_data_over_folds(model_names[0:3], out_dir, df.cell_num, X, y)\n",
    "\n",
    "# ensemble\n",
    "d_full_cv_probs = d_full_cv[[k for k in d_full_cv.keys() if 'proba' in k]]\n",
    "preds_soft = d_full_cv_probs.sum(axis=1) / d_full_cv_probs.shape[1]\n",
    "print(preds_soft)\n",
    "\n",
    "for score_name in train.scorers.keys():\n",
    "    print(score_name, train.scorers[score_name](y_full_cv, preds_soft > 0.5))\n",
    "# metrics.accuracy_score(y_ensemble, preds_soft>0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
