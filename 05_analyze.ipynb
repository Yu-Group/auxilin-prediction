{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "plt.style.use('dark_background')\n",
    "import mat4py\n",
    "import pandas as pd\n",
    "import data_tracks\n",
    "import models\n",
    "from sklearn.model_selection import KFold\n",
    "from colorama import Fore\n",
    "import pickle as pkl\n",
    "import viz\n",
    "from style import *\n",
    "import analyze_helper\n",
    "import train\n",
    "import data_tracks\n",
    "\n",
    "outcome_def = 'y_consec_sig'\n",
    "out_dir = 'results/outcome=y_consec_sig'\n",
    "results = analyze_helper.load_results(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at prediction metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "r = r[[k for k in r if not '_f' in k]]\n",
    "# r = r[r.index.str.contains('ros')] # only use random sampling\n",
    "r = r.sort_values(by=['accuracy', 'balanced_accuracy'], ascending=False)\n",
    "# r.style.background_gradient(cmap='viridis', axis=None) # all values on same cmap\n",
    "r.style.background_gradient(cmap='viridis', axis=0) # columns differently colored\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at feat importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])\n",
    "r = r[r.accuracy > 0.65]\n",
    "keys = [k for k in r if '_f' in k]\n",
    "keys_remapped = {k: k.replace('_f', '') for k in keys}\n",
    "r = r[keys].rename(columns=keys_remapped)\n",
    "# r = r.sort_values('lifetime')\n",
    "# r = r[r.index.str.contains('39')]\n",
    "r = r[~r.index.str.contains('=11')]\n",
    "# r = r[~r.index.str.contains('=7')]\n",
    "r = r[~r.index.str.contains('=15')]\n",
    "r = r.rename(columns={'mean_square_displacement': 'msd', 'total_displacement': 'td'})\n",
    "\n",
    "# r = r[r.index.str.contains('11')]\n",
    "# r = r[r2.index.str.contains('ros')]\n",
    "# r = r[r.index.str.contains('none')]\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "\n",
    "\n",
    "def rank(r):\n",
    "    '''Rank feature importances appropriately\n",
    "    '''\n",
    "    r = r.abs()\n",
    "    r = r.rank(axis=1, ascending=False, method='min')\n",
    "    return r\n",
    "# \n",
    "r = rank(r)\n",
    "r = r.reindex(r.mean().sort_values(ascending=True).index, axis=1) # sort cols by mean rank\n",
    "\n",
    "idxs = r.index\n",
    "r.insert(0, 'acc', results.loc[idxs]['accuracy'])\n",
    "r = r.sort_values('acc', ascending=False)\n",
    "\n",
    "subset = list(r.keys())\n",
    "subset.remove('acc')\n",
    "r.fillna(0).style.background_gradient(cmap='viridis_r', axis=1, subset=subset) # rows differently colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mlp2_15_ros_select_rf=3' 'svm_25_ros_select_rf=11'\n",
      " 'svm_25_ros_select_rf=5' 'mlp2_45_ros_select_rf=7'\n",
      " 'svm_15_ros_select_rf=5' 'mlp2_25_ros_select_rf=7'\n",
      " 'mlp2_25_ros_select_rf=3' 'svm_25_ros_select_rf=3'\n",
      " 'svm_45_ros_select_rf=5' 'svm_15_ros_select_rf=11']\n"
     ]
    }
   ],
   "source": [
    "df = data_tracks.get_data()\n",
    "n = df.shape[0]\n",
    "\n",
    "# normalize and store\n",
    "X = df[data_tracks.get_feature_names(df)]\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "ks = list(X.keys())\n",
    "norms = {ks[i]: {'mu': X_mean[i], 'std': X_std[i]} for i in range(len(ks))}\n",
    "X = (X - X_mean) / X_std\n",
    "y = df[outcome_def].values\n",
    "\n",
    "\n",
    "# split testing data based on cell num\n",
    "cv_idx = data_tracks.cell_nums_train[0]\n",
    "idxs_test = df.cell_num.isin([cv_idx]) # this is the cv set for model's trained in the first cv fold\n",
    "X_test, Y_test = X[idxs_test], y[idxs_test]\n",
    "\n",
    "# look at best models\n",
    "r = results\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "idx = np.array(r.index)\n",
    "print(idx[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = idx[:12]\n",
    "model_name = idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_full_cv, y_full_cv = analyze_helper.get_data_over_folds(model_name, out_dir, df.cell_num, X, y)\n",
    "preds, preds_proba = d_full_cv[model_name], d_full_cv[model_name + '_proba']\n",
    "\n",
    "results_individual = pkl.load(open(oj(out_dir, f'{model_name}.pkl'), 'rb'))\n",
    "assert np.mean(preds==y_full_cv) == np.average(results_individual['cv']['accuracy'], weights=results_individual['num_pts_by_fold_cv']), 'did not properly load model/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = 'dt_15_none_select_rf=7'\n",
    "results_individual = pkl.load(open(oj(out_dir, f'{model_name}.pkl'), 'rb'))\n",
    "preds, preds_proba = analyze_helper.analyze_individual_results(results_individual, X_test, Y_test, \n",
    "                                                print_results=False, plot_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_lifetime(X_test, preds, preds_proba, Y_test, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_spatially(df, idxs_test, preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_outliers(X_test, preds, Y_test, num_feats_reduced=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_fp = preds > Y_test\n",
    "idxs_fn = preds < Y_test\n",
    "print('fp')\n",
    "viz.viz_biggest_errs(df['X'][idxs_test][idxs_fp], Y_test[idxs_fp], preds[idxs_fp], preds_proba[idxs_fp])    \n",
    "print('fn')\n",
    "viz.viz_biggest_errs(df['X'][idxs_test][idxs_fn], Y_test[idxs_fn], preds[idxs_fn], preds_proba[idxs_fn])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['mlp2_11_none', 'svm_35_none', 'logistic_4_none', 'rf_9_none']\n",
    "# model_names = ['mlp2_11_none', 'mlp2_9_none', 'mlp2_4_none']\n",
    "df_preds = analyze_helper.load_results_many_models(out_dir, model_names, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**venn-diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = []\n",
    "for model_name in model_names[:3]:\n",
    "    args = np.argwhere(df_preds[model_name + '_errs'])\n",
    "    sets.append(set(args.flatten().tolist()))\n",
    "    \n",
    "plt.figure(dpi=300)\n",
    "plt.title('venn diagram of shared errors')\n",
    "venn3(sets, model_names[:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quick check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_full_cv, y_full_cv = analyze_helper.get_data_over_folds(model_names[0:3], out_dir, df.cell_num, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_accuracy 0.6775520859231267\n",
      "accuracy 0.6921587608906099\n",
      "precision 0.7877813504823151\n",
      "recall 0.7248520710059172\n",
      "f1 0.7550077041602465\n",
      "roc_auc 0.6775520859231268\n",
      "precision_recall_curve (array([0.65440465, 0.78778135, 1.        ]), array([1.        , 0.72485207, 0.        ]), array([False,  True]))\n",
      "roc_curve (array([0.       , 0.3697479, 1.       ]), array([0.        , 0.72485207, 1.        ]), array([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# ensemble\n",
    "preds_soft = d_full_cv.sum(axis=1) / d_full_cv.shape[1]\n",
    "\n",
    "for score_name in train.scorers.keys():\n",
    "    print(score_name, train.scorers[score_name](y_full_cv, preds_soft > 0.5))\n",
    "# metrics.accuracy_score(y_ensemble, preds_soft>0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
