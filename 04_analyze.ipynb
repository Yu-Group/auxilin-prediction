{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "plt.style.use('dark_background')\n",
    "import mat4py\n",
    "import pandas as pd\n",
    "import data_tracks\n",
    "import models\n",
    "from matplotlib_venn import venn3, venn2\n",
    "from sklearn.model_selection import KFold\n",
    "from colorama import Fore\n",
    "import pickle as pkl\n",
    "import viz\n",
    "from style import *\n",
    "import analyze_helper\n",
    "import train\n",
    "from sklearn import decomposition\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "outcome_def = 'y_consec_thresh'\n",
    "out_dir = 'results/jan16_9'\n",
    "results = analyze_helper.load_results(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all models\n",
    "**baseline stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid:\t\t1862 aux+ / 7594 (0.245)\n",
      "----------------------------------------\n",
      "no_hotspots:\t1662 aux+ / 7394 (0.225)\n",
      "----------------------------------------\n",
      "aux_early:\t   0 aux+ /    0 (nan)\n",
      "aux_late:\t 739 aux+ / 1413 (0.523)\n",
      "aux_valid:\t 923 aux+ / 5981 (0.154)\n",
      "----------------------------------------\n",
      "lifetime<=35:\t4625 aux+ / 5027 (0.920)\n",
      "lifetime>=243:\t   1 aux- /    1 (1.000)\n",
      "remaining:\t 520 aux+ /  953 (0.546)\n",
      "----------------------------------------\n",
      "predicted acc:\t\t\t  0.759\n",
      "total acc:\t\t\t  0.894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/accounts/projects/vision/chandan/abc-image-understanding/viz.py:291: RuntimeWarning: invalid value encountered in long_scalars\n",
      "  print(f'aux_early:\\t{m[\"num_aux_pos_early\"]:>4.0f} aux+ / {m[\"num_peaks_early\"]:>4} ({m[\"num_aux_pos_early\"]/m[\"num_peaks_early\"]:.3f})')\n"
     ]
    }
   ],
   "source": [
    "viz.print_metadata(acc=results.accuracy.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at prediction metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "r = r[[k for k in r if not '_f' in k]]\n",
    "# r = r[r.index.str.contains('ros')] # only use random sampling\n",
    "r = r.sort_values(by=['accuracy', 'balanced_accuracy'], ascending=False)\n",
    "# r.style.background_gradient(cmap='viridis', axis=None) # all values on same cmap\n",
    "r.style.background_gradient(cmap='viridis', axis=0) # columns differently colored\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at feat importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])\n",
    "r = r[r.accuracy > np.percentile(r.accuracy, 90)]\n",
    "keys = [k for k in r if '_f' in k]\n",
    "keys_remapped = {k: k.replace('_f', '') for k in keys}\n",
    "r = r[keys].rename(columns=keys_remapped)\n",
    "# r = r.sort_values('lifetime')\n",
    "# r = r[r.index.str.contains('39')]\n",
    "r = r[~r.index.str.contains('=11')]\n",
    "r = r[~r.index.str.contains('=7')]\n",
    "r = r[~r.index.str.contains('=15')]\n",
    "r = r.rename(columns={'mean_square_displacement': 'msd', 'total_displacement': 'td'})\n",
    "\n",
    "# r = r[r.index.str.contains('11')]\n",
    "# r = r[r2.index.str.contains('ros')]\n",
    "# r = r[r.index.str.contains('none')]\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "\n",
    "\n",
    "def rank(r):\n",
    "    '''Rank feature importances appropriately\n",
    "    '''\n",
    "    r = r.abs()\n",
    "    r = r.rank(axis=1, ascending=False, method='min')\n",
    "    return r\n",
    "# \n",
    "r = rank(r)\n",
    "# r = r.reindex(r.mean().sort_values(ascending=True).index, axis=1) # sort cols by mean rank\n",
    "\n",
    "idxs = r.index\n",
    "r.insert(0, 'acc', results.loc[idxs]['accuracy'])\n",
    "r = r.sort_values('acc', ascending=False)\n",
    "r = r.reindex(r.iloc[0].sort_values(ascending=True).index, axis=1) # sort cols by mean rank\n",
    "subset = list(r.keys())\n",
    "subset.remove('acc')\n",
    "\n",
    "\n",
    "def color_vals(val):\n",
    "    colors = {\n",
    "        1: cb, 2: cp, 3: cr,\n",
    "    }\n",
    "    if not val in colors:\n",
    "        color = 'maroon'\n",
    "    else:\n",
    "        color = colors[val]\n",
    "    return 'color: %s' % color\n",
    "\n",
    "r.fillna(\"\").style.applymap(color_vals, subset=subset)\n",
    "# r.fillna(0).style.background_gradient(cmap='viridis_r', axis=1, subset=subset) # rows differently colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.759 mlp2_15_ros=1.2_select_rf=5\n",
      "0.757 svm_15_ros=1.2_select_lasso=15\n",
      "0.757 svm_15_ros=1.2_select_rf=15\n",
      "0.757 svm_25_ros=1.2_select_lasso=15\n",
      "0.757 svm_30_ros=1.2_select_lasso=15\n"
     ]
    }
   ],
   "source": [
    "df = data_tracks.get_data()\n",
    "n = df.shape[0]\n",
    "\n",
    "# normalize and store\n",
    "def normalize(df):\n",
    "    X = df[data_tracks.get_feature_names(df)]\n",
    "    X_mean = X.mean()\n",
    "    X_std = X.std()\n",
    "    ks = list(X.keys())\n",
    "    norms = {ks[i]: {'mu': X_mean[i], 'std': X_std[i]} for i in range(len(ks))}\n",
    "    X = (X - X_mean) / X_std\n",
    "    y = df[outcome_def].values\n",
    "    return X, y, norms\n",
    "X, y, norms = normalize(df)\n",
    "\n",
    "\n",
    "\n",
    "# split testing data based on cell num\n",
    "# cv_idx = data_tracks.cell_nums_train[0]\n",
    "# idxs_test = df.cell_num.isin([cv_idx]) # this is the cv set for model's trained in the first cv fold\n",
    "# X_test, Y_test = X[idxs_test], y[idxs_test]\n",
    "\n",
    "# look at best models\n",
    "r = results\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "idx = np.array(r.index)\n",
    "accs = np.array(r.accuracy)\n",
    "for i in range(5):\n",
    "    print(f'{accs[i]:.3f}', idx[i])\n",
    "model_name = 'svm_15_ros=1.2_select_rf=3' # idx[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_full_cv, idxs_cv = analyze_helper.get_data_over_folds(model_name, out_dir, df.cell_num, X, y)\n",
    "y_full_cv = df[outcome_def].iloc[idxs_cv].values.astype(np.int)\n",
    "preds = d_full_cv[model_name].values\n",
    "preds_proba = d_full_cv[model_name + '_proba'].values\n",
    "\n",
    "results_individual = pkl.load(open(oj(out_dir, f'{model_name}.pkl'), 'rb'))\n",
    "assert np.mean(preds==y_full_cv) == np.average(results_individual['cv']['accuracy'], weights=results_individual['num_pts_by_fold_cv']), 'did not properly load model/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_fp = preds > y_full_cv\n",
    "idxs_fn = preds < y_full_cv\n",
    "print(f'frac false pos {idxs_fp.sum() / preds.shape[0]:0.3f}')\n",
    "print(f'frac false neg {idxs_fn.sum() / preds.shape[0]:0.3f}')\n",
    "viz.viz_biggest_errs(df, df['X'].iloc[idxs_cv][idxs_fp], df['Y'].iloc[idxs_cv][idxs_fp], \n",
    "                     y_full_cv[idxs_fp], preds[idxs_fp], preds_proba[idxs_fp])    \n",
    "plt.show()\n",
    "print('false neg')\n",
    "viz.viz_biggest_errs(df, df['X'].iloc[idxs_cv][idxs_fn], df['Y'].iloc[idxs_cv][idxs_fn], \n",
    "                     y_full_cv[idxs_fn], preds[idxs_fn], preds_proba[idxs_fn])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inter-cell accs [0.74 0.78 0.72 0.77 0.69]\n"
     ]
    }
   ],
   "source": [
    "print('inter-cell accs', np.array(results_individual['cv']['accuracy']).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_to_end = df.iloc[idxs_fp]['lifetime'] - df.iloc[idxs_fp]['Y_peak_idx'] - 1\n",
    "frac_to_end = 1 - df.iloc[idxs_fp]['Y_peak_idx'] / df.iloc[idxs_fp]['lifetime']\n",
    "print(np.unique(dist_to_end, return_counts=True))\n",
    "plt.hist(frac_to_end, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading + preprocessing data...\n",
      "\tloading tracks...\n",
      "\tpreprocessing data...\n",
      "\tadding features...\n",
      "dict features not added!\n",
      "hotspots acc:\t\t\t0.6385542168674698\n"
     ]
    }
   ],
   "source": [
    "# calculate hotspots acc\n",
    "results_individual = pkl.load(open(oj(out_dir, f'{model_name}.pkl'), 'rb'))\n",
    "df = data_tracks.get_data(use_processed=False, remove_hotspots=False, save_processed=False)\n",
    "df = df[df['hotspots']==1]\n",
    "X, y, norms = normalize(df)\n",
    "\n",
    "preds, pred_proba = analyze_helper.analyze_individual_results(results_individual, \n",
    "                                               X, y, print_results=False, \n",
    "                                               plot_results=False, model_cv_fold=1)\n",
    "print(f'hotspots acc:\\t\\t\\t{np.mean(preds==df[outcome_def])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'X_max' # lifetime, X_min, fall, X_max\n",
    "viz.viz_errs_1d(X.iloc[idxs_cv], preds, preds_proba, y_full_cv, norms, key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key1 = 'X_max' # fall, x_pos, pc1\n",
    "key2 = 'X_min' # X_min, y_pos, pc2\n",
    "viz.viz_errs_2d(df, idxs_cv, preds, y_full_cv, key1=key1, key2=key2)\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_outliers_venn(X.iloc[idxs_cv], preds, y_full_cv, num_feats_reduced=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=200)\n",
    "prob_true, prob_pred = calibration_curve(y_full_cv, preds_proba, normalize=False, n_bins=10, strategy='quantile')\n",
    "plt.plot(prob_true, prob_pred, '.')\n",
    "plt.plot([0, 1], [0, 1], alpha=0.3, color='gray')\n",
    "plt.xlabel('true prob')\n",
    "plt.ylabel('predicted prob')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at pcs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pca\n",
    "pca = decomposition.PCA(whiten=True)\n",
    "pca.fit(X.iloc[idxs_cv])\n",
    "viz.plot_pcs(pca, X.iloc[idxs_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=2, whiten=True)\n",
    "X_reduced = pca.fit_transform(X.iloc[idxs_cv])\n",
    "plt.figure(dpi=200)\n",
    "ys_neg = y[idxs_cv] == 0\n",
    "plt.plot(X_reduced[:, 0][ys_neg], X_reduced[:, 1][ys_neg], 'o', \n",
    "         color=cr, alpha=0.3, markeredgewidth=0, ms=2)\n",
    "plt.plot(X_reduced[:, 0][~ys_neg], X_reduced[:, 1][~ys_neg], 'o', \n",
    "         color=cb, alpha=0.3, markeredgewidth=0, ms=2)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_names = ['mlp2_11_none', 'svm_35_none', 'logistic_4_none', 'rf_9_none']\n",
    "# model_names = ['mlp2_11_none', 'mlp2_9_none', 'mlp2_4_none']\n",
    "model_names = idx[:3]\n",
    "d_full_cv, idxs_cv = analyze_helper.get_data_over_folds(model_names, out_dir, df.cell_num, X, y)\n",
    "y_full_cv = df[outcome_def].iloc[idxs_cv].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ensemble err**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_accuracy 0.7307451262767688\n",
      "accuracy 0.7394094993581515\n",
      "precision 0.7415254237288136\n",
      "recall 0.8120649651972158\n",
      "f1 0.7751937984496123\n",
      "roc_auc 0.7307451262767688\n",
      "precision_recall_curve (array([0.55327343, 0.74152542, 1.        ]), array([1.        , 0.81206497, 0.        ]), array([False,  True]))\n",
      "roc_curve (array([0.        , 0.35057471, 1.        ]), array([0.        , 0.81206497, 1.        ]), array([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# ensemble\n",
    "d_full_cv_probs = d_full_cv[[k for k in d_full_cv.keys() if 'proba' in k]]\n",
    "preds_soft = d_full_cv_probs.sum(axis=1) / d_full_cv_probs.shape[1]\n",
    "\n",
    "for score_name in train.scorers.keys():\n",
    "    print(score_name, train.scorers[score_name](y_full_cv, preds_soft > 0.5))\n",
    "# metrics.accuracy_score(y_ensemble, preds_soft>0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**venn-diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = []\n",
    "for model_name in model_names:\n",
    "    args = np.argwhere(d_full_cv[model_name] != y_full_cv)\n",
    "    sets.append(set(args.flatten().tolist()))\n",
    "    \n",
    "plt.figure(dpi=300)\n",
    "plt.title('venn diagram of shared errors')\n",
    "venn3(sets, model_names[:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
