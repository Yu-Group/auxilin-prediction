{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join as oj\n",
    "from sklearn.feature_extraction.image import extract_patches_2d\n",
    "from sklearn import metrics\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from matplotlib_venn import venn3\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.datasets import make_classification\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from copy import deepcopy\n",
    "from sklearn import metrics\n",
    "plt.style.use('dark_background')\n",
    "import mat4py\n",
    "import pandas as pd\n",
    "import data_tracks\n",
    "import models\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import KFold\n",
    "from colorama import Fore\n",
    "import pickle as pkl\n",
    "import viz\n",
    "from style import *\n",
    "import analyze_helper\n",
    "import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'results/outcome=y_thresh'\n",
    "results = analyze_helper.load_results(out_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at prediction metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "r = r[[k for k in r if not '_f' in k]]\n",
    "# r = r[r.index.str.contains('ros')] # only use random sampling\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "# r.style.background_gradient(cmap='viridis', axis=None) # all values on same cmap\n",
    "r.style.background_gradient(cmap='viridis', axis=0) # columns differently colored\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**look at feat importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results\n",
    "# r.style.apply(viz.highlight_max, subset=[k for k in r if not 'std' in k])\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "keys = [k for k in r if '_f' in k]\n",
    "keys_remapped = {k: k.replace('_f', '') for k in keys}\n",
    "r = r[keys].rename(columns=keys_remapped)\n",
    "# r = r.sort_values('lifetime')\n",
    "# r = r[r.index.str.contains('39')]\n",
    "# r = r[r.index.str.contains('11')]\n",
    "r = r.rename(columns={'mean_square_displacement': 'msd', 'total_displacement': 'td'})\n",
    "\n",
    "r = r[r.index.str.contains('11')]\n",
    "# r = r[r2.index.str.contains('ros')]\n",
    "r = r[r.index.str.contains('none')]\n",
    "r = r[[k for k in r if not 'std' in k]]\n",
    "\n",
    "\n",
    "def rank(r):\n",
    "    '''Rank feature importances appropriately\n",
    "    '''\n",
    "    r = r.abs()\n",
    "    r = r.rank(axis=1, ascending=False, method='min')\n",
    "    return r\n",
    "# \n",
    "r = rank(r)\n",
    "r = r.reindex(r.mean().sort_values(ascending=True).index, axis=1) # sort cols by mean rank\n",
    "idxs = r.index\n",
    "r.insert(0, 'acc', results.loc[idxs]['accuracy'])\n",
    "\n",
    "subset = list(r.keys())\n",
    "subset.remove('acc')\n",
    "r.fillna(0).style.background_gradient(cmap='viridis_r', axis=1, subset=subset) # rows differently colored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analyze model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_tracks.get_data()\n",
    "n = df.shape[0]\n",
    "\n",
    "# make logistic data\n",
    "# feat_names = ['X_max', 'lifetime', 'total_displacement', 'mean_square_displacement']\n",
    "# feat_names = ['X_max', 'lifetime', 'total_displacement', 'mean_square_displacement', 'rise', 'fall']\n",
    "# feat_names = ['X_max', 'lifetime', 'total_displacement', 'mean_square_displacement', \n",
    "#               'rise', 'fall', 'X_std', 'X_min', 'X_mean', 'max_diff', 'min_diff']\n",
    "scs = [f'sc_{i}' for i in range(12)]\n",
    "bases = [f'sc_{i}' for i in range(12)] + [f'nmf_{i}' for i in range(12)]\n",
    "patches = ['up_max', 'down_max', 'left_max', 'right_max']\n",
    "feat_names = ['X_max', 'lifetime', 'total_displacement', 'mean_square_displacement', \n",
    "              'rise', 'fall', 'X_std', 'X_min', 'X_mean', 'max_diff', 'min_diff'] + bases + patches\n",
    "X = df[feat_names]\n",
    "\n",
    "# normalize and store\n",
    "X_mean = X.mean()\n",
    "X_std = X.std()\n",
    "norms = {\n",
    "    feat_names[i]: {'mu': X_mean[i], 'std': X_std[i]} for i in range(len(feat_names))\n",
    "}\n",
    "X = (X - X_mean) / X_std\n",
    "y = df['y_thresh'].values\n",
    "\n",
    "\n",
    "# split testing data based on cell num\n",
    "idxs_test = df.cell_num.isin([6])\n",
    "X_test, Y_test = X[idxs_test], y[idxs_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = pkl.load(open('results/classify_outcome=y_consec_sig/rf_ros_11.pkl', 'rb'))\n",
    "results_individual = pkl.load(open('results/outcome=y_thresh/mlp2_11_none.pkl', 'rb'))\n",
    "preds, preds_proba = analyze_helper.analyze_individual_results(results_individual, X_test, Y_test, \n",
    "                                                print_results=False, plot_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_lifetime(X_test, preds, preds_proba, Y_test, norms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz.viz_errs_spatially(df, idxs_test, preds, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_fp = preds > Y_test\n",
    "idxs_fn = preds < Y_test\n",
    "viz.viz_biggest_errs(X_traces_test[idxs_fp], Y_test[idxs_fp], preds[idxs_fp], preds_proba[idxs_fp])    \n",
    "viz.viz_biggest_errs(X_traces_test[idxs_fn], Y_test[idxs_fn], preds[idxs_fn], preds_proba[idxs_fn])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### look at many models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['voting_mlp+svm+rf_23_none' 'mlp2_11_none' 'mlp2_9_none' 'svm_35_none'\n",
      " 'svm_23_none' 'voting_mlp+svm+rf_9_none' 'voting_mlp+svm+rf_11_none'\n",
      " 'voting_mlp+svm+rf_35_none' 'svm_9_none' 'svm_11_none']\n"
     ]
    }
   ],
   "source": [
    "r = results\n",
    "r = r.sort_values('accuracy', ascending=False)\n",
    "idx = np.array(r.index)\n",
    "print(idx[:10])\n",
    "# model_names = ['mlp2_11_none', 'svm_35_none', 'logistic_4_none', 'rf_9_none']\n",
    "# model_names = ['mlp2_11_none', 'mlp2_9_none', 'mlp2_4_none']\n",
    "model_names = idx[:12] #['mlp2_11_none', 'mlp2_9_none', 'mlp2_4_none']\n",
    "\n",
    "d = {}\n",
    "for i, model_name in enumerate(model_names):\n",
    "    results_individual = pkl.load(open(f'results/outcome=y_thresh/{model_name}.pkl', 'rb'))\n",
    "    preds, preds_proba = analyze_helper.analyze_individual_results(results_individual, X_test, Y_test, \n",
    "                                                print_results=False, plot_results=False)\n",
    "    d[model_name] = preds\n",
    "    d[model_name + '_proba'] = preds_proba\n",
    "    d[model_name + '_errs'] = preds!=Y_test\n",
    "df_preds = pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**venn-diagram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = []\n",
    "for model_name in model_names[:3]:\n",
    "    args = np.argwhere(df_preds[model_name + '_errs'])\n",
    "    sets.append(set(args.flatten().tolist()))\n",
    "    \n",
    "plt.figure(dpi=300)\n",
    "plt.title('venn diagram of shared errors')\n",
    "venn3(sets, model_names[:3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ensemble err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**quick check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class imbalance 0.59\n",
      "acc 0.62\n",
      "roc_auc 0.56\n",
      "balanced acc: 0.56\n"
     ]
    }
   ],
   "source": [
    "df_probas = df_preds[[k for k in df_preds if 'proba' in k]]\n",
    "preds_test_new = df_probas.sum(axis=1) / (df_probas.shape[1])\n",
    "preds_labels = preds_test_new > 0.5\n",
    "\n",
    "print(f'class imbalance {np.mean(Y_test):0.2f}')\n",
    "print(f'acc {np.mean(Y_test == preds_labels):0.2f}')\n",
    "print(f'roc_auc {metrics.roc_auc_score(Y_test, preds_labels):0.2f}')\n",
    "print(f'balanced acc: {metrics.balanced_accuracy_score(Y_test, preds_labels):0.2f}') #' r2 {metrics.r2_score(Y_train, preds):0.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**properly ensemble best models (over folds + models)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split testing data based on cell num\n",
    "d = {}\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "cell_nums_train = [1, 2, 3, 4, 5]\n",
    "Y_vals = []\n",
    "\n",
    "for i, model_name in enumerate(model_names):\n",
    "    results_individual = pkl.load(open(f'results/outcome=y_thresh/{model_name}.pkl', 'rb'))\n",
    "\n",
    "    fold_num = 0\n",
    "    for cv_idx, cv_val_idx in kf.split(cell_nums_train):\n",
    "        # get sample indices\n",
    "        idxs_cv = df.cell_num.isin(cv_idx + 1)\n",
    "        idxs_val_cv = df.cell_num.isin(cv_val_idx + 1)\n",
    "        X_train_cv, Y_train_cv = X[idxs_cv], y[idxs_cv]\n",
    "        X_val_cv, Y_val_cv = X[idxs_val_cv], y[idxs_val_cv]\n",
    "\n",
    "        # get predictions\n",
    "        preds, preds_proba = analyze_helper.analyze_individual_results(results_individual, X_val_cv, Y_val_cv, \n",
    "                                                    print_results=False, plot_results=False, model_cv_fold=fold_num)\n",
    "        \n",
    "        d[f'{model_name}_{fold_num}'] = preds\n",
    "        d[f'{model_name}_{fold_num}_proba'] = preds_proba\n",
    "        \n",
    "        if i == 0:\n",
    "            Y_vals.append(Y_val_cv)\n",
    "        \n",
    "        fold_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {}\n",
    "y_ensemble = np.hstack(Y_vals)\n",
    "for model_name in model_names:\n",
    "    d2[model_name] = np.hstack([d[k] for k in d.keys() if model_name in k and 'proba' in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = pd.DataFrame.from_dict(d2)\n",
    "preds_soft = d3.sum(axis=1) / d3.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "balanced_accuracy 0.6003315593359517\n",
      "accuracy 0.6786050895381716\n",
      "precision 0.7011764705882353\n",
      "recall 0.8726207906295754\n",
      "f1 0.7775603392041747\n",
      "roc_auc 0.6003315593359517\n",
      "precision_recall_curve (array([0.64373233, 0.70117647, 1.        ]), array([1.        , 0.87262079, 0.        ]), array([False,  True]))\n",
      "roc_curve (array([0.        , 0.67195767, 1.        ]), array([0.        , 0.87262079, 1.        ]), array([2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "for score_name in train.scorers.keys():\n",
    "    print(score_name, train.scorers[score_name](y_ensemble, preds_soft > 0.5))\n",
    "# metrics.accuracy_score(y_ensemble, preds_soft>0.5) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
